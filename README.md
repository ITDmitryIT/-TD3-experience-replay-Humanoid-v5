# TD3 с PER для Humanoid-v5

Реализация алгоритма TD3 (Twin Delayed Deep Deterministic Policy Gradient) с приоритетным experience replay (PER) для обучения гуманоида ходить в среде Humanoid-v5 из Gymnasium.

## Задача
Цель проекта — сравнить производительность и стабильность алгоритмов обучения с подкреплением (TD3 с PER, TD3 без PER, DDPG, PPO) в сложной среде с непрерывными действиями. Задача: обучить гуманоида двигаться, максимизируя награду за устойчивость и скорость.

## Что сделано
- Реализованы алгоритмы TD3 (с PER и без), DDPG и PPO.
- Проведено обучение на 1000 эпизодов в среде Humanoid-v5.
- Выполнена визуализация наград и дисперсии за последние 100 эпизодов.
- Составлена таблица сравнения производительности (средняя награда, дисперсия, время обучения).
- Сформулированы выводы о преимуществах TD3 с PER (средняя награда ~320, дисперсия на 30% ниже DDPG).

## Технологии
- **Язык**: Python 3.12
- **Библиотеки**: PyTorch, NumPy, Matplotlib, Gymnasium (MuJoCo)
- **Почему этот стек**: PyTorch для гибкости нейронных сетей, Gymnasium для стандартизированной среды RL, Matplotlib для визуализации.

## Установка и запуск
1. Установите зависимости:
   ```bash
   pip install torch numpy matplotlib 'gymnasium[mujoco]'
